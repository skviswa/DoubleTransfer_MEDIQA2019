{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from shutil import copyfile, move\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from data_utils.mediqa_utils import submit, eval_model,mediqa_name_list\n",
    "from data_utils.label_map import DATA_META, GLOBAL_MAP, DATA_TYPE, DATA_SWAP, TASK_TYPE, generate_decoder_opt\n",
    "from data_utils.log_wrapper import create_logger\n",
    "from data_utils.utils import set_environment\n",
    "from data_utils.mediqa2019_evaluator_allTasks_final import eval_mediqa_official\n",
    "from mt_dnn.batcher import BatchGen\n",
    "from mt_dnn.model import MTDNNModel\n",
    "from bert.modeling import BertModel\n",
    "from bert.modeling import BertConfig\n",
    "#import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = json.load(open('data_config.json', 'r'))\n",
    "train_config = json.load(open('train_config.json', 'r'))\n",
    "model_config = json.load(open('model_config.json', 'r'))\n",
    "config = {}\n",
    "config.update(data_config)\n",
    "config.update(train_config)\n",
    "config.update(model_config)\n",
    "print(len(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### None configs by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['init_config'] = None\n",
    "config['resume_scoring'] = None\n",
    "config['test_datasets'] = None\n",
    "config['predict_split'] = None\n",
    "config['mediqa_pairloss'] = None\n",
    "config['batch_size_eval'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_flag = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['mtl_observe_datasets'] = config['mtl_observe_datasets'].split(',')\n",
    "if config['float_medquad']:\n",
    "    TASK_TYPE['medquad'] = 1\n",
    "    DATA_META['medquad'] = 1\n",
    "\n",
    "config['float_target'] = False\n",
    "\n",
    "if config['batch_size_eval'] is None:\n",
    "    config['batch_size_eval'] = config['batch_size']\n",
    "\n",
    "output_dir = config['output_dir']\n",
    "data_dir = config['data_dir']\n",
    "if config['test_datasets'] is None:\n",
    "    config['test_datasets'] = config['train_datasets']\n",
    "# args.train_datasets = args.train_datasets.split(',')\n",
    "# args.test_datasets = args.test_datasets.split(',')\n",
    "if len(config['train_datasets'])==1:\n",
    "    config['mtl_observe_datasets'] = config['train_datasets']\n",
    "config['external_datasets'] = config['external_datasets'].split(',') if config['external_datasets'] != '' else []\n",
    "config['train_datasets'] = config['train_datasets'] + config['external_datasets']\n",
    "\n",
    "# pprint(config)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_dir = os.path.abspath(output_dir)\n",
    "\n",
    "set_environment(config['seed'], cuda_flag)\n",
    "log_path = os.path.join(output_dir, config['log_file'])\n",
    "logger =  create_logger(__name__, to_disk=True, log_file=log_path)\n",
    "logger.info(config['answer_opt'])\n",
    "\n",
    "#TODO: Check whether this should be loaded from Minio\n",
    "tasks_config = {}\n",
    "if os.path.exists(config['task_config_path']):\n",
    "    with open(config['task_config_path'], 'r') as reader:\n",
    "        tasks_config = json.loads(reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json dump utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(path, data):\n",
    "    with open(path ,'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dev and test dataset lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_test_dataset(opt, nclass_list):\n",
    "    \n",
    "    opt['label_size'] = nclass_list #','.join([str(l) for l in nclass_list])\n",
    "    logger.info(opt['label_size'])\n",
    "    dev_data_list = []\n",
    "    test_data_list = []\n",
    "    \n",
    "    for dataset in opt['test_datasets']:\n",
    "        prefix = dataset.split('_')[0]\n",
    "        task_id = tasks_class[DATA_META[prefix]] if opt['mtl_opt'] > 0 else tasks[prefix]\n",
    "        task_type = TASK_TYPE[prefix]\n",
    "\n",
    "        pw_task = False\n",
    "\n",
    "        assert prefix in DATA_TYPE\n",
    "        data_type = DATA_TYPE[prefix]\n",
    "\n",
    "        if opt['predict_split'] is not None:\n",
    "            dev_path = os.path.join(data_dir, '{}_{}.json'.format(dataset, \n",
    "                args.predict_split))\n",
    "        else:\n",
    "            dev_path = os.path.join(data_dir, '{}_dev.json'.format(dataset))\n",
    "        \n",
    "        dev_data = None\n",
    "        if os.path.exists(dev_path):\n",
    "            \n",
    "            #TODO: Check whether this should be loaded from Minio\n",
    "            dev_data = BatchGen(BatchGen.load(dev_path, False, pairwise=pw_task, maxlen=opt['max_seq_len'],\n",
    "                                            opt=opt, dataset=dataset),\n",
    "                                  batch_size=opt['batch_size_eval'],\n",
    "                                  gpu=cuda_flag, is_train=False,\n",
    "                                  task_id=task_id,\n",
    "                                  maxlen=opt['max_seq_len'],\n",
    "                                  pairwise=pw_task,\n",
    "                                  data_type=data_type,\n",
    "                                  task_type=task_type,\n",
    "                                  dataset_name=dataset)\n",
    "        dev_data_list.append(dev_data)\n",
    "\n",
    "        test_path = os.path.join(data_dir, '{}_test.json'.format(dataset))\n",
    "        test_data = None\n",
    "        if os.path.exists(test_path):\n",
    "            \n",
    "            #TODO: Check whether this should be loaded from Minio\n",
    "            test_data = BatchGen(BatchGen.load(test_path, False, pairwise=pw_task, \n",
    "                                            maxlen=opt['max_seq_len'],opt=opt, dataset=dataset),\n",
    "                                  batch_size=opt['batch_size_eval'],\n",
    "                                  gpu=cuda_flag, is_train=False,\n",
    "                                  task_id=task_id,\n",
    "                                  maxlen=opt['max_seq_len'],\n",
    "                                  pairwise=pw_task,\n",
    "                                  data_type=data_type,\n",
    "                                  task_type=task_type,\n",
    "                                  dataset_name=dataset)\n",
    "        test_data_list.append(test_data)\n",
    "        \n",
    "        return {'dev_list': dev_data_list, 'test_list': test_data_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(opt, data_dir):\n",
    "    \n",
    "#     opt = vars(args)\n",
    "    # update data dir\n",
    "    opt['data_dir'] = data_dir\n",
    "#     batch_size = args.batch_size\n",
    "    batch_size = opt['batch_size']\n",
    "    train_data_list = []\n",
    "    tasks = {}\n",
    "    tasks_class = {}\n",
    "    nclass_list = []\n",
    "    decoder_opts = []\n",
    "    dropout_list = []\n",
    "\n",
    "    for dataset in opt['train_datasets']:\n",
    "        prefix = dataset.split('_')[0]\n",
    "        if prefix in tasks: continue\n",
    "        assert prefix in DATA_META\n",
    "        assert prefix in DATA_TYPE\n",
    "        data_type = DATA_TYPE[prefix]\n",
    "        nclass = DATA_META[prefix]\n",
    "        task_id = len(tasks)\n",
    "        \n",
    "        if opt['mtl_opt'] > 0:\n",
    "            task_id = tasks_class[nclass] if nclass in tasks_class else len(tasks_class)\n",
    "\n",
    "        task_type = TASK_TYPE[prefix]\n",
    "        pw_task = False\n",
    "\n",
    "        dopt = generate_decoder_opt(prefix, opt['answer_opt'])\n",
    "        if task_id < len(decoder_opts):\n",
    "            decoder_opts[task_id] = min(decoder_opts[task_id], dopt)\n",
    "        else:\n",
    "            decoder_opts.append(dopt)\n",
    "\n",
    "        if prefix not in tasks:\n",
    "            tasks[prefix] = len(tasks)\n",
    "            if opt['mtl_opt'] < 1: nclass_list.append(nclass)\n",
    "\n",
    "        if (nclass not in tasks_class):\n",
    "            tasks_class[nclass] = len(tasks_class)\n",
    "            if opt['mtl_opt'] > 0: nclass_list.append(nclass)\n",
    "\n",
    "        dropout_p = opt['dropout_p']\n",
    "        if tasks_config and prefix in tasks_config:\n",
    "            dropout_p = tasks_config[prefix]\n",
    "        dropout_list.append(dropout_p)\n",
    "\n",
    "        train_path = os.path.join(data_dir, '{}_train.json'.format(dataset))\n",
    "        logger.info('Loading {} as task {}'.format(train_path, task_id))\n",
    "        \n",
    "        #TODO: Check whether this should be loaded from Minio\n",
    "        train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=opt['max_seq_len'], \n",
    "                                        opt=opt, dataset=dataset),\n",
    "                                batch_size=batch_size,\n",
    "                                dropout_w=opt['dropout_w'],\n",
    "                                gpu=cuda_flag,\n",
    "                                task_id=task_id,\n",
    "                                maxlen=opt['max_seq_len'],\n",
    "                                pairwise=pw_task,\n",
    "                                data_type=data_type,\n",
    "                                task_type=task_type,\n",
    "                                dataset_name=dataset)\n",
    "        train_data.reset()\n",
    "        train_data_list.append(train_data)\n",
    "\n",
    "    opt['answer_opt'] = decoder_opts\n",
    "    opt['tasks_dropout_p'] = dropout_list\n",
    "    \n",
    "    return {'train_list': train_data_list, 'nclass_list': n_class_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update state dict utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_dict(opt, model_path, state_dict):\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        #TODO: Check whether this should be loaded from Minio\n",
    "        state_dict = torch.load(model_path)\n",
    "        \n",
    "        if opt['init_config'] is not None: # load huggingface model\n",
    "            config = json.load(open(args.init_config))\n",
    "            state_dict={'config':config, 'state':state_dict}\n",
    "            \n",
    "        if arg_config['finetune']:\n",
    "            # only resume config and state\n",
    "            del_keys=set(state_dict.keys())-set(['config','state'])\n",
    "            for key in del_keys:\n",
    "                del state_dict[key]\n",
    "                \n",
    "            # Parameterize this\n",
    "            resume_configs=json.load(open('config/resume_configs.json'))\n",
    "            del_keys=set(state_dict['config'].keys())-set(resume_configs)\n",
    "            \n",
    "            for key in del_keys:\n",
    "                del state_dict['config'][key]\n",
    "                \n",
    "            if opt['resume_scoring'] is not None: \n",
    "                \n",
    "                for key in state_dict['state'].keys():\n",
    "                    if 'scoring_list.0' in key:\n",
    "                        state_dict['state'][key]=state_dict['state'][key.replace('0',str(opt['resume_scoring']))]\n",
    "                        # other scorings will be deleted during loading process, since finetune only has one task\n",
    "                        \n",
    "            elif not opt['retain_scoring']:\n",
    "                del_keys = [k for k in state_dict['state'] if 'scoring_list' in k]\n",
    "                \n",
    "                for key in del_keys:                    \n",
    "                    print('deleted previous weight:',key)\n",
    "                    del state_dict['state'][key]\n",
    "        \n",
    "        config = state_dict['config']\n",
    "        config['attention_probs_dropout_prob'] = opt['bert_dropout_p']\n",
    "        config['hidden_dropout_prob'] = opt['bert_dropout_p']\n",
    "        opt.update(config)\n",
    "    else:\n",
    "        logger.error('#' * 20)\n",
    "        logger.error('Could not find the init model!\\n The parameters will be initialized randomly!')\n",
    "        logger.error('#' * 20)\n",
    "        config = BertConfig(vocab_size_or_config_json_file=30522).to_dict()\n",
    "        opt.update(config)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_test_eval(config, model, dev_data_list, test_data_list, dev_split):\n",
    "        \n",
    "        this_performance = {}\n",
    "        for idx, dataset in enumerate(config['test_datasets']):\n",
    "            prefix = dataset.split('_')[0]\n",
    "            dev_data = dev_data_list[idx]\n",
    "            if dev_data is not None:\n",
    "                dev_metrics, dev_predictions, scores, golds, dev_ids= eval_model(model, dev_data, dataset=prefix,\n",
    "                                                                                 use_cuda=cuda_flag)\n",
    "                #TODO: Check whether this should be saved to Minio\n",
    "                score_file = os.path.join(output_dir, '{}_{}_scores_{}.json'.format(dataset, dev_split, epoch))\n",
    "                results = {'metrics': dev_metrics, 'predictions': dev_predictions, 'uids': dev_ids, 'scores': scores}\n",
    "                dump(score_file, results)\n",
    "                \n",
    "                #TODO: Check whether this should be saved to Minio\n",
    "                official_score_file = os.path.join(output_dir, '{}_{}_scores_{}.csv'.format(dataset, dev_split, epoch))\n",
    "                submit(official_score_file, results,dataset_name=prefix, threshold=2.0+config['mediqa_score_offset'])\n",
    "                \n",
    "                if prefix in mediqa_name_list:\n",
    "                    logger.warning('self test numbers:{}'.format(dev_metrics))\n",
    "                    \n",
    "                    #TODO: Check whether this should be loaded from Minio\n",
    "                    if '_' in dataset:\n",
    "                        affix = dataset.split('_')[1]\n",
    "                        ground_truth_path=os.path.join(config['data_root'],'mediqa/task3_qa/gt_{}_{}.csv'.format(dev_split,affix))\n",
    "                    else:\n",
    "                        ground_truth_path=os.path.join(config['data_root'],'mediqa/task3_qa/gt_{}.csv'.format(dev_split))\n",
    "                    \n",
    "                    official_result=eval_mediqa_official(pred_path=official_score_file, ground_truth_path=ground_truth_path, \n",
    "                        eval_qa_more=config['mediqa_eval_more'])\n",
    "                    \n",
    "                    logger.warning(\"MediQA dev eval result:{}\".format(official_result))\n",
    "                    \n",
    "                    if config['mediqa_eval_more']:\n",
    "                        dev_metrics={'ACC':official_result['score']*100,'Spearman':official_result['score_secondary']*100,\n",
    "                                    'F1':dev_metrics['F1'], 'MRR':official_result['meta']['MRR'], 'MAP':official_result['MAP'],\n",
    "                                    'P@1':official_result['meta']['P@1']}\n",
    "                    else:\n",
    "                        dev_metrics={'ACC':official_result['score']*100,'Spearman':official_result['score_secondary']*100}\n",
    "\n",
    "                for key, val in dev_metrics.items():\n",
    "                    logger.warning(\"Task {0} -- epoch {1} -- Dev {2}: {3:.3f}\".format(dataset, epoch, key, val))\n",
    "            if config['predict_split'] is not None:\n",
    "                continue\n",
    "            print('args.mtl_observe_datasets:',config['mtl_observe_datasets'], dataset)\n",
    "            if dataset in config['mtl_observe_datasets']:\n",
    "                this_performance[dataset]=np.mean([val for val in dev_metrics.values()])\n",
    "            test_data = test_data_list[idx]\n",
    "            if test_data is not None:\n",
    "                test_metrics, test_predictions, scores, golds, test_ids= eval_model(model, test_data, dataset=prefix,\n",
    "                                                                                 use_cuda=cuda_flag, with_label=False)\n",
    "                for key, val in test_metrics.items():\n",
    "                    logger.warning(\"Task {0} -- epoch {1} -- Test {2}: {3:.3f}\".format(dataset, epoch, key, val))\n",
    "                score_file = os.path.join(output_dir, '{}_test_scores_{}.json'.format(dataset, epoch))\n",
    "                results = {'metrics': test_metrics, 'predictions': test_predictions, 'uids': test_ids, 'scores': scores}\n",
    "                dump(score_file, results)\n",
    "                # if dataset in mediqa_name_list:\n",
    "                official_score_file = os.path.join(output_dir, '{}_test_scores_{}.csv'.format(dataset, epoch))\n",
    "                submit(official_score_file, results,dataset_name=prefix, threshold=2.0+config['mediqa_score_offset'])\n",
    "                logger.info('[new test scores saved.]')\n",
    "                \n",
    "        return this_performance    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train util per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_epoch(model, train_data_list, epoch, config):\n",
    "    \n",
    "        logger.warning('At epoch {}'.format(epoch))\n",
    "        if epoch==0 and config['freeze_bert_first']:\n",
    "            model.network.freeze_bert()\n",
    "            logger.warning('Bert freezed.')\n",
    "        if epoch==1 and config['freeze_bert_first']:\n",
    "            model.network.unfreeze_bert()\n",
    "            logger.warning('Bert unfreezed.')\n",
    "        start = datetime.now()\n",
    "        all_indices=[]\n",
    "        if len(config['external_datasets'])> 0 and config['external_include_ratio'] > 0:\n",
    "            main_indices = []\n",
    "            extra_indices = []\n",
    "            for data_idx,batcher in enumerate(train_data_list):\n",
    "                if batcher. dataset_name not in config['external_datasets']:\n",
    "                    main_indices += [data_idx] * len(batcher)\n",
    "                else:\n",
    "                    extra_indices += [data_idx] * len(batcher)\n",
    "\n",
    "            random_picks=int(min(len(main_indices) * config['external_include_ratio'], len(extra_indices)))\n",
    "            extra_indices = np.random.choice(extra_indices, random_picks, replace=False)\n",
    "            if config['mix_opt'] > 0:\n",
    "                extra_indices = extra_indices.tolist()\n",
    "                random.shuffle(extra_indices)\n",
    "                all_indices = extra_indices + main_indices\n",
    "            else:\n",
    "                all_indices = main_indices + extra_indices.tolist()\n",
    "        else:\n",
    "            for i in range(1, len(train_data_list)):\n",
    "                all_indices += [i] * len(train_data_list[i])\n",
    "            if config['mix_opt'] > 0:\n",
    "                random.shuffle(all_indices)\n",
    "            all_indices += [0] * len(train_data_list[0])\n",
    "        if config['mix_opt'] < 1:\n",
    "            random.shuffle(all_indices)\n",
    "        if config['test_mode']:\n",
    "            all_indices=all_indices[:2]\n",
    "        if config['predict_split'] is not None:\n",
    "            all_indices=[]\n",
    "            dev_split=config['predict_split']\n",
    "        else:\n",
    "            dev_split='dev'\n",
    "\n",
    "        for i in range(len(all_indices)):\n",
    "            task_id = all_indices[i]\n",
    "            batch_meta, batch_data= next(all_iters[task_id])\n",
    "            model.update(batch_meta, batch_data)\n",
    "            if (model.updates) % config['log_per_updates'] == 0 or model.updates == 1:\n",
    "                logger.info('Task [{0:2}] updates[{1:6}] train loss[{2:.5f}] remaining[{3}]'.format(task_id,\n",
    "                    model.updates, model.train_loss.avg,\n",
    "                    str((datetime.now() - start) / (i + 1) * (len(all_indices) - i - 1)).split('.')[0]))\n",
    "        \n",
    "        return {'dev_split': dev_split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model saving util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, config, output_dir, epoch, best_dataset_performance):\n",
    "    \n",
    "    #TODO: Check whether this needs to be saved directly to Minio\n",
    "    if not config['not_save_model']:\n",
    "\n",
    "        model_name = 'model_last.pt' if config['save_last'] else 'model_{}.pt'.format(epoch) \n",
    "        model_file = os.path.join(output_dir, model_name)\n",
    "\n",
    "        if config['save_last'] and os.path.exists(model_file):\n",
    "            model_temp=os.path.join(output_dir, 'model_secondlast.pt')\n",
    "            copyfile(model_file, model_temp)\n",
    "\n",
    "        model.save(model_file)\n",
    "\n",
    "        if config['save_best'] and best_epoch==epoch:\n",
    "            best_path = os.path.join(output_dir,'best_model.pt')\n",
    "            copyfile(model_file,best_path)\n",
    "\n",
    "            for dataset in config['mtl_observe_datasets']:\n",
    "                if best_dataset_performance[dataset]['epoch'] == epoch:\n",
    "                    best_path = os.path.join(output_dir,'best_model_{}.pt'.format(dataset))\n",
    "                    copyfile(model_file, best_path)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, data_dir, output_dir):\n",
    "    \n",
    "    logger.info('Launching the MT-DNN training')\n",
    "    logger.info('#' * 20)\n",
    "    logger.info(opt)\n",
    "    logger.info('#' * 20)\n",
    "    \n",
    "    train_data_dict = get_train_dataset(config, data_dir)\n",
    "    train_data_list = train_data_dict['train_list']\n",
    "    nclass_list = train_data_dict['nclass_list']\n",
    "    \n",
    "    dev_test_dict = get_dev_test_dataset(config, nclass_list)\n",
    "    dev_data_list = dev_test_dict['dev_list']\n",
    "    test_data_list = dev_test_dict['test_list']\n",
    "\n",
    "    all_iters =[iter(item) for item in train_data_list]\n",
    "    all_lens = [len(bg) for bg in train_data_list]\n",
    "    num_all_batches = config['epochs'] * sum(all_lens)\n",
    "\n",
    "    if len(config['external_datasets']) > 0 and config['external_include_ratio'] > 0:\n",
    "        num_in_domain_batches = config['epochs']* sum(all_lens[:-len(config['.external_datasets'])])\n",
    "        num_all_batches = num_in_domain_batches * (1 + config['external_include_ratio'])\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    model_path = config['init_checkpoint']\n",
    "    state_dict = None\n",
    "    update_state_dict(config, model_path, state_dict)\n",
    "    \n",
    "\n",
    "    model = MTDNNModel(config, state_dict=state_dict, num_train_step=num_all_batches)\n",
    "    ####model meta str\n",
    "    headline = '############# Model Arch of MT-DNN #############'\n",
    "    ###print network\n",
    "    # logger.info('\\n{}\\n{}\\n'.format(headline, model.network))\n",
    "    \n",
    "    #TODO: Check whether it needs to be saved to Minio\n",
    "    # dump config\n",
    "    config_file = os.path.join(output_dir, 'config.json')\n",
    "    with open(config_file, 'w', encoding='utf-8') as writer:\n",
    "        writer.write('{}\\n'.format(json.dumps(config)))\n",
    "        writer.write('\\n{}\\n{}\\n'.format(headline, model.network))\n",
    "\n",
    "    logger.info(\"Total number of params: {}\".format(model.total_param))\n",
    "\n",
    "    if config['freeze_layers'] > 0:\n",
    "        model.network.freeze_layers(config['freeze_layers'])\n",
    "\n",
    "    if cuda_flag:\n",
    "        model.cuda()\n",
    "        \n",
    "    best_epoch=-1\n",
    "    best_performance=0 \n",
    "    best_dataset_performance={dataset:{'perf':0,'epoch':-1} for dataset in config['mtl_observe_datasets']}\n",
    "    \n",
    "     for epoch in range(config['epochs']):\n",
    "        \n",
    "        train_dict = train_per_epoch(model, train_data_list, epoch, config)\n",
    "        for train_data in train_data_list:\n",
    "            train_data.reset()\n",
    "\n",
    "        this_performance = dev_test_eval(config, model, dev_data_list, test_data_list, train_dict['dev_split'])\n",
    "        print('this_performance:',this_performance)\n",
    "    \n",
    "        if config['predict_split'] is not None:\n",
    "            break\n",
    "        epoch_performance = sum([val for val in this_performance.values()])\n",
    "        if epoch_performance>best_performance:\n",
    "            print('changed:',epoch_performance,best_performance)\n",
    "            best_performance=epoch_performance\n",
    "            best_epoch=epoch\n",
    "\n",
    "        for dataset in config['mtl_observe_datasets']:\n",
    "            if best_dataset_performance[dataset]['perf'] < this_performance[dataset]:\n",
    "                best_dataset_performance[dataset]= {'perf':this_performance[dataset],\n",
    "                                                   'epoch':epoch} \n",
    "\n",
    "\n",
    "        print('current best:',best_performance,'at epoch', best_epoch)\n",
    "        \n",
    "        save_model(model, config, output_dir, epoch, best_dataset_performance)\n",
    "    \n",
    "    return {'model': model, \n",
    "            'train_data_list': train_data_list, \n",
    "            'dev_data_list': dev_data_list, \n",
    "            'test_data_list': test_data_list, \n",
    "            'best_dataset_performance': best_dataset_performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtenv",
   "language": "python",
   "name": "dtenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
